import matplotlib.pyplot as plt 
import numpy as np 
import random
import math
import scipy.stats as sp
import pdb

MAX_ITER = 1000
EPS = 10**(-50)

def read_toy_dataset():
	"""
	returns toy dataset array (np.array)
	"""
	text_file = open("/Users/kunal/Documents/2016-17/Spring/kondor/hw1/toydata.txt", "r")
	return np.array([line.split() for line in text_file.readlines()], dtype=np.float64)

def compute_likelihood(cond_prob):
	"""
	computes the loglikelihood
	"""
	return np.array([p.max() for p in cond_prob]).prod()


def N(X,mu,S):
	"""
	Finds a vector of p-values of the rows of X being generated by a N(mu,S) mv gaussian
	"""
	d = X.shape[1]
	det = np.linalg.det(S)
	if det == 0:
		raise NameError("Covariance matrix is singular")
	c = (2*math.pi)**(-d/2) * det**(-1/2)
	X_n = X - mu
	return c * np.exp(-0.5 * np.diag(np.dot(X_n, np.dot(np.linalg.inv(S), X_n.T))))

def prob_matrix(X,mus,Ss):
	"""
	Finds the matrix with i,j element P(xi | zi=j)
	"""
	probs = []
	if mus.shape[0] != Ss.shape[0]:
		raise NameError("mus and Ss don't have agreeing shapes")
	for i in range(mus.shape[0]):
		probs.append(N(X,mus[i],Ss[i]))
	return np.array(probs).T

def cond_probs(X,mus,Ss,pi):
	"""
	Finds the matrix of conditional probabilities P(zi=j)
	"""
	if pi.shape[0] != mus.shape[0]:
		raise NameError("mus and pi don't have agreeing shapes")
	p = pi * prob_matrix(X,mus,Ss)
	return (p.T / p.sum(axis=1)).T

def update_pi(X,cond_prob):
	"""
	Updates the values of pi
	"""
	return cond_prob.sum(axis=0) / X.shape[0]

def update_mu(X, cond_prob, nk):
	"""
	Updates the values of mu
	"""
	return np.dot(cond_prob.T, X) / nk[:, np.newaxis]
	#mus = []
	#for j in range(cond_prob.shape[1]):
	#	mus.append(np.array([cond_prob[i,j]*X[i] for i in 
	#		range(X.shape[0])]).sum(axis=0) / cond_prob[:,j].sum())
	#return np.array(mus)

def update_sigma(X, cond_prob, mu, nk):
	"""
	Updates the value of S
	"""
	#There is probably a better way to vectorize this
	
	Sigmas = []
	for j in range(mu.shape[0]):
		diff = X - mu[j]
		Sigmas.append(np.dot(cond_prob[:, j] * diff.T, diff) / nk[j])
	return np.array(Sigmas)
	#Sigmas = []
	#for j in range(mu.shape[0]):
	#	Sigmas.append(np.array([cond_prob[i,j]*np.outer(X[i],X[i]) 
	#		for i in range(500)]).sum(axis=0) / cond_prob[:,j].sum())
	#return np.array(Sigmas)

def gaussian_em(X, k):
	"""
	Implements the gaussian em model. Wasn't sure how to do this, so comments inline
	"""
	#Initializes pi to uniform, mus to k random points, and Ss all as identity mattrices
	pi = np.repeat(1/k, k)
	mus = np.array(random.sample(list(X), k))
	Ss = np.array([np.identity(X.shape[1]) for i in range(k)])
	cp = cond_probs(X, mus, Ss, pi)
	likelihoods = [compute_likelihood(cp)]

	for i in range(MAX_ITER):
		nk = cp.sum(axis=0) + 10*np.finfo(cp.dtype).eps
		pi = update_pi(X, cp)
		Ss = update_sigma(X, cp, mus, nk)
		mus = update_mu(X, cp, nk)
		cp = cond_probs(X, mus, Ss, pi)
		likelihood = compute_likelihood(cp)
		if likelihood - likelihoods[-1] < 0:
			print("Algorithm converged after {} iterations".format(i+1))
			return cp.argmax(-1), mus, likelihoods
		likelihoods.append(likelihood)
	return cp.argmax(-1), mus, likelihoods

def plot_assignment(points, assignments, mus):
	"""
	plots (using pyplot) a color coded 2D plot of assignments
	"""
	col_list = ['r', 'g', 'b', 'y', 't']
	for i in range(max(set(assignments))+1):
		i_pts = points[assignments == i]
		plt.scatter(i_pts[:,0], i_pts[:,1], c=col_list[i])
	plt.scatter(mus[:, 0], mus[:, 1], c='r', s=100)

def gauss_em_plot(points, k, iters=20):
	"""
	plots (using pyplot) the distortion trend of iters number of independe
	kmeans runs, using the defined k_means function
	"""
	for i in range(iters):
		a, m, d = gaussian_em(points, 3)
		plt.plot(np.log(d))
	plt.ylabel('log-likelihood')
	plt.xlabel('iteration')
